{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:25:15.318692Z",
     "start_time": "2024-04-09T14:25:11.864672Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip sub-THUCNews.csv.xz to sub-THUCNews.csv\n",
      "load sub-THUCNews finish.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 108\u001B[0m\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;66;03m# print(result)\u001B[39;00m\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m--> 108\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_set\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m show_confusion_matrix(r)\n\u001B[0;32m    110\u001B[0m show_statistics(r)\n",
      "Cell \u001B[1;32mIn[1], line 84\u001B[0m, in \u001B[0;36mpredict\u001B[1;34m(testfile)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(testfile):\n\u001B[0;32m     82\u001B[0m     label_test, _, input_test, _ \u001B[38;5;241m=\u001B[39m load_data(testfile)\n\u001B[1;32m---> 84\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel/lstm.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m     tok \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel/token.pickle\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     87\u001B[0m     test_seq \u001B[38;5;241m=\u001B[39m tok\u001B[38;5;241m.\u001B[39mtexts_to_sequences(input_test)\n",
      "File \u001B[1;32mE:\\textclassification_nlp\\venv\\lib\\site-packages\\keras\\engine\\saving.py:458\u001B[0m, in \u001B[0;36mallow_read_from_gcs.<locals>.load_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    456\u001B[0m         os\u001B[38;5;241m.\u001B[39mremove(tmp_filepath)\n\u001B[0;32m    457\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[1;32m--> 458\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m load_function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\textclassification_nlp\\venv\\lib\\site-packages\\keras\\engine\\saving.py:550\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(filepath, custom_objects, compile)\u001B[0m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m H5Dict\u001B[38;5;241m.\u001B[39mis_supported_type(filepath):\n\u001B[0;32m    549\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m H5Dict(filepath, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m h5dict:\n\u001B[1;32m--> 550\u001B[0m         model \u001B[38;5;241m=\u001B[39m \u001B[43m_deserialize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh5dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_objects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcompile\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    551\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(filepath, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwrite\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(filepath\u001B[38;5;241m.\u001B[39mwrite):\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_function\u001B[39m(h5file):\n",
      "File \u001B[1;32mE:\\textclassification_nlp\\venv\\lib\\site-packages\\keras\\engine\\saving.py:242\u001B[0m, in \u001B[0;36m_deserialize_model\u001B[1;34m(h5dict, custom_objects, compile)\u001B[0m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNo model found in config.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 242\u001B[0m model_config \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(\u001B[43mmodel_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m    243\u001B[0m model \u001B[38;5;241m=\u001B[39m model_from_config(model_config, custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects)\n\u001B[0;32m    244\u001B[0m model_weights_group \u001B[38;5;241m=\u001B[39m h5dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_weights\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "# %load test.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import sequence\n",
    "import pickle\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, Activation, Dropout, Input\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "try:\n",
    "    from lstm.pre_process import load_data\n",
    "except ImportError:\n",
    "    from pre_process import load_data\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# %matplotlib inline\n",
    "from texttable import Texttable\n",
    "\n",
    "sns.set(font=\"simhei\")\n",
    "\n",
    "max_len = 300\n",
    "\n",
    "test_set = \"sub-THUCNews\"\n",
    "\n",
    "l = ['体育', '娱乐', '家居', '彩票', '房产', '教育', '时尚', '时政', '星座', '游戏', '社会', '科技', '股票', '财经']\n",
    "CLASSES = {l[i]: i for i, _ in enumerate(l)}\n",
    "re_CLASSES = {i:l[i] for i, _ in enumerate(l)}\n",
    "\n",
    "\n",
    "def show_confusion_matrix(evaluation):\n",
    "    f, ax = plt.subplots(figsize=(12, 10))\n",
    "    df = pd.DataFrame(confusion_matrix(evaluation['y_true'], evaluation['y_pred']), columns=CLASSES, index=CLASSES)\n",
    "    # print(df)\n",
    "    ax = sns.heatmap(df, annot=True, robust=True, fmt=\"d\", cmap=\"Oranges\", linewidths=0.5)\n",
    "    # ax.set_ylim(0.5, 0.5)\n",
    "    print(ax.get_ylim())\n",
    "    ax.set_ylim(14, 0)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.tick_params(direction='in')\n",
    "    label_y = ax.get_yticklabels()\n",
    "    plt.setp(label_y, rotation=360, horizontalalignment='right', fontsize=13)\n",
    "    label_x = ax.get_xticklabels()\n",
    "    plt.setp(label_x, horizontalalignment='center', fontsize=13)\n",
    "    f.savefig(\"confusion_matrix.png\", bbox_inches='tight', dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_statistics(evaluation):\n",
    "    p, r, f1, s = precision_recall_fscore_support(evaluation['y_true'], evaluation['y_pred'])\n",
    "    avg_p = np.average(p, weights=s)\n",
    "    avg_r = np.average(r, weights=s)\n",
    "    avg_f1 = np.average(f1, weights=s)\n",
    "    total_s = np.sum(s)\n",
    "    df1 = pd.DataFrame({'类别': l, '准确率': p, '召回率': r, 'F-measure': f1, '数量': s})\n",
    "    df2 = pd.DataFrame({'类别': ['总体'], '准确率': [avg_p], '召回率': [avg_r], 'F-measure': [avg_f1], '数量': [total_s]})\n",
    "    df2.index = [14]\n",
    "    df = pd.concat([df1, df2])\n",
    "    tb = Texttable()\n",
    "    print(df)\n",
    "    tb.set_cols_align(['l', 'r', 'r', 'r', 'r'])\n",
    "    # tb.set_cols_dtype(['t','i','i'])\n",
    "    tb.header(df.columns.get_values())\n",
    "    tb.add_rows(df.values, header=False)\n",
    "    print(tb.draw())\n",
    "\n",
    "\n",
    "def predict(testfile):\n",
    "    label_test, _, input_test, _ = load_data(testfile)\n",
    "\n",
    "    model = load_model('model/lstm.h5')\n",
    "    tok = pickle.load(open('model/token.pickle', 'rb'))\n",
    "\n",
    "    test_seq = tok.texts_to_sequences(input_test)\n",
    "    # print(input_test)\n",
    "    test_seq_mat = sequence.pad_sequences(test_seq, maxlen=max_len)\n",
    "    pre_result = model.predict(test_seq_mat)\n",
    "    # print(pre)\n",
    "    confidence = dict()\n",
    "    y_pred = []\n",
    "    for i,sent in enumerate(pre_result):\n",
    "        p = re_CLASSES[np.argmax(sent)]\n",
    "        # label = label_test[i]\n",
    "        y_pred.append(p)\n",
    "\n",
    "        # print(label, p)\n",
    "\n",
    "    # max_index = np.argmax(pre)\n",
    "    # max_type = l[max_index]\n",
    "    result = {\"y_true\": label_test.to_list(), \"y_pred\": y_pred}\n",
    "    # print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "r = predict(test_set)\n",
    "show_confusion_matrix(r)\n",
    "show_statistics(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:25:21.762027Z",
     "start_time": "2024-04-09T14:25:21.724028200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\textclassification_nlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\textclassification_nlp\\venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:26:13.128451600Z",
     "start_time": "2024-04-09T14:26:13.072713300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mdirname(\u001B[38;5;18;43m__file__\u001B[39;49m)))\n",
      "\u001B[1;31mNameError\u001B[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(os.path.dirname(__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:32:09.128701900Z",
     "start_time": "2024-04-09T14:32:09.062075200Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 67\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m max_type, confidence\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 67\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;28mprint\u001B[39m(t)\n",
      "Cell \u001B[1;32mIn[6], line 30\u001B[0m, in \u001B[0;36mpredict\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(text):\n\u001B[1;32m---> 30\u001B[0m     stopwords \u001B[38;5;241m=\u001B[39m [i\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstopwords.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mu8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mread()]\n\u001B[0;32m     32\u001B[0m     token \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m jieba\u001B[38;5;241m.\u001B[39mcut(text) \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stopwords])\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28mprint\u001B[39m(token)\n",
      "File \u001B[1;32mE:\\textclassification_nlp\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    308\u001B[0m     )\n\u001B[1;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'stopwords.txt'"
     ]
    }
   ],
   "source": [
    "# %load lstm/predict.py\n",
    "# predict\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import sequence\n",
    "import pickle\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "try:\n",
    "    from lstm.pre_process import load_data\n",
    "except ImportError:\n",
    "    from .pre_process import load_data\n",
    "\n",
    "max_len = 1000\n",
    "csv_data = \"THUCNews\"\n",
    "test_data = \"sub-THUCNews\"\n",
    "\n",
    "# label, data, tokenize, length = load_data()\n",
    "# input_data, input_label, _, _ = train_test_split(tokenize, label, test_size=0.9)\n",
    "\n",
    "text = \"\"\"\n",
    "北京时间12月18日，2019年东亚杯冠军产生，韩国队1-0击败日本队，以3战全胜的战绩历史上第5次获得东亚杯的冠军，成为首支东亚杯3连冠球队！虽然世界杯和亚洲杯的表现不如对手，但这一次韩国队找回场子，此外在去年亚运会以及今年世青赛，韩国国奥与韩国国青都曾击败日本队，3条战线都击败对手。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    stopwords = [i.strip() for i in open(\"stopwords.txt\", encoding='u8').read()]\n",
    "\n",
    "    token = \" \".join([i for i in jieba.cut(text) if i not in stopwords])\n",
    "    print(token)\n",
    "\n",
    "    model = load_model('model/lstm.h5')\n",
    "    tok = pickle.load(open('model/token.pickle', 'rb'))\n",
    "\n",
    "    test_seq = tok.texts_to_sequences([token])\n",
    "    test_seq_mat = sequence.pad_sequences(test_seq, maxlen=max_len)\n",
    "\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    le_label_test = le.fit_transform(input_label).reshape(-1,1)\n",
    "    \n",
    "    # 分类标签转为 one hot\n",
    "    ohe = OneHotEncoder()\n",
    "    le_label_test = ohe.fit_transform(le_label_test).toarray()\n",
    "    \n",
    "    \n",
    "    score, ac = model.evaluate(test_seq_mat, le_label_test, batch_size=128)\n",
    "    print(score, ac)\n",
    "    \"\"\"\n",
    "    l = ['体育', '娱乐', '家居', '彩票', '房产', '教育', '时尚', '时政', '星座', '游戏', '社会', '科技', '股票', '财经']\n",
    "    confidence = dict()\n",
    "\n",
    "    pre = model.predict(test_seq_mat)\n",
    "    for i, j in enumerate(l):\n",
    "        confidence[j] = pre[0][i]\n",
    "\n",
    "    max_index = np.argmax(pre)\n",
    "    max_type = l[max_index]\n",
    "    confidence = {k: v for k, v in sorted(confidence.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return max_type, confidence\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = predict(text)\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
