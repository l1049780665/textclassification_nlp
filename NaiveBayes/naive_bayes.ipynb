{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本预处理\n",
    "- 分词\n",
    "- 去停用词\n",
    "- 保存train.txt,test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T10:20:53.783281300Z",
     "start_time": "2023-12-06T10:20:53.745004400Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjieba\u001B[39;00m\n\u001B[0;32m      9\u001B[0m STOP_SIGNALS \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\u3000\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\xa0\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m,}\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_stop_words\u001B[39m(stop_words_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstopword.txt\u001B[39m\u001B[38;5;124m'\u001B[39m,stop_signals\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\u3000\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\xa0\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m,}):\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import jieba\n",
    "\n",
    "\n",
    "STOP_SIGNALS = {'\\n','\\u3000','\\xa0',' ',}\n",
    "def load_stop_words(stop_words_path='stopword.txt',stop_signals={'\\n','\\u3000','\\xa0',' ',}):\n",
    "    with open(stop_words_path,encoding='UTF-8-sig') as f:\n",
    "        s_stop_words = set()\n",
    "        for line in f:\n",
    "            s_stop_words.add(line.strip())\n",
    "        s_stop_words.update(stop_signals)\n",
    "        return s_stop_words\n",
    "    \n",
    "def tokenization(text):\n",
    "    '''\n",
    "    给定一段文本，返回分词结果\n",
    "    '''\n",
    "    return jieba.cut(text)\n",
    "\n",
    "def process(save_path, sub_path, s_stop_words,category, l_files):\n",
    "    res_f = open(save_path,'a',encoding='UTF-8-sig')\n",
    "    for file in l_files:\n",
    "            with open(sub_path+'/'+file,'r',encoding='utf-8') as f:\n",
    "                token_res = []          \n",
    "                for word in tokenization(f.read()):\n",
    "                    if word not in s_stop_words:\n",
    "                        token_res.append(word)\n",
    "                res_f.write(category+' '+ ' '.join(token_res)+'\\n')\n",
    "                \n",
    "    res_f.close()\n",
    "    \n",
    "def text_preprocessing(data_path = 'data'):\n",
    "    '''\n",
    "    读取文件数据，进行分词，去停用，保存\n",
    "    '''\n",
    "    print('preprocessing...')\n",
    "    s_stop_words = load_stop_words()\n",
    "    l_categories = ['财经','彩票','房产','股票','家居','教育','科技','社会','时尚','时政','体育','星座','游戏','娱乐']\n",
    "    for category in l_categories:\n",
    "        # 生成训练集数据\n",
    "        sub_path = data_path + '/'+category\n",
    "        l_files = os.listdir(sub_path)\n",
    "        process('train.txt',sub_path, s_stop_words,category, l_files[:int(0.85*len(l_files))])  # 生成训练集数据\n",
    "        # 生成测试集数据\n",
    "        process('test.txt',sub_path, s_stop_words,category, l_files[int(0.85*len(l_files)):])  # 生成测试集数据\n",
    "    \n",
    "    print('process finished.')\n",
    "    \n",
    "text_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计信息\n",
    "- 统计每个类别文档数量\n",
    "- 统计单词-类别文档数量\n",
    "- 统计每个文档中词频信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T10:16:49.138492700Z",
     "start_time": "2023-12-06T10:16:49.024232400Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 37\u001B[0m\n\u001B[0;32m     29\u001B[0m                     d_words_count[word][label] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124md_words_count\u001B[39m\u001B[38;5;124m'\u001B[39m:d_words_count,\n\u001B[0;32m     33\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124md_doc_count\u001B[39m\u001B[38;5;124m'\u001B[39m:d_doc_count,\n\u001B[0;32m     34\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml_words_counters\u001B[39m\u001B[38;5;124m'\u001B[39m:l_words_counters\n\u001B[0;32m     35\u001B[0m     }\n\u001B[1;32m---> 37\u001B[0m res \u001B[38;5;241m=\u001B[39m info_statis()\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124md_words_count\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m女足\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124md_doc_count\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m体育\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Cell \u001B[1;32mIn[2], line 9\u001B[0m, in \u001B[0;36minfo_statis\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m d_doc_count \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m      8\u001B[0m l_words_counters \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain.txt\u001B[39m\u001B[38;5;124m'\u001B[39m,encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUTF-8-sig\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m f:\n\u001B[0;32m     11\u001B[0m         label, content \u001B[38;5;241m=\u001B[39m line\u001B[38;5;241m.\u001B[39mstrip()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32mD:\\PythonDevelop\\Anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    280\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    282\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    283\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    284\u001B[0m     )\n\u001B[1;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'train.txt'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "l_categories = ['财经','彩票','房产','股票','家居','教育','科技','社会','时尚','时政','体育','星座','游戏','娱乐']\n",
    "\n",
    "def info_statis():\n",
    "    d_words_count = {}\n",
    "    d_doc_count = {}\n",
    "    l_words_counters = []\n",
    "    with open('train.txt',encoding='UTF-8-sig') as f:\n",
    "        for line in f:\n",
    "            label, content = line.strip().split(' ',1)\n",
    "            words_counter = collections.Counter(content.split(' '))\n",
    "            l_words_counters.append(words_counter)\n",
    "            if label in d_doc_count:\n",
    "                d_doc_count[label] += 1\n",
    "            else:\n",
    "                d_doc_count[label] = 1\n",
    "    \n",
    "            for word in words_counter.keys():\n",
    "                if word in d_words_count and label in d_words_count[word].keys():\n",
    "                    # 已存在相应单词\n",
    "                    d_words_count[word][label] += 1  # 属于label类且对应item的文档数量\n",
    "\n",
    "                elif word in d_words_count and label not in d_words_count[word].keys():\n",
    "                    d_words_count[word][label] = 1\n",
    "                \n",
    "                else:\n",
    "                    d_words_count[word] = {}\n",
    "                    d_words_count[word][label] = 1\n",
    "    \n",
    "    return {\n",
    "        'd_words_count':d_words_count,\n",
    "        'd_doc_count':d_doc_count,\n",
    "        'l_words_counters':l_words_counters\n",
    "    }\n",
    "\n",
    "res = info_statis()\n",
    "print(res['d_words_count']['女足'])\n",
    "print(res['d_doc_count']['体育'])\n",
    "print(res['l_words_counters'][:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取\n",
    "- 特征选择  \n",
    "    依据互信息选择每个类别前50个特征词作为特征项\n",
    "- 特征权重计算    \n",
    "    依据TF-IDF计算每个特征的权重  \n",
    "- 特征和权重矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['黄金', '美元', '高', '风险', '金价', '银行', '预期', '分析师', '因素', '期货', '盎司', '黄金价格', '避险', '货币', '金', '新高', '波动', '通胀', '支撑', '央行', '金融危机', '债务', '高位', '业内人士', '人民币', '吨', '交易所', '最低', '刺激', '贵金属', '担忧', '创下', '纽约', '首席', '黄金市场', '复苏', '债券', '高点', '实物', '收益率', '克', '涨', '走高', 'ETF', '保值', '贷款', '纪录', '回调', '跌', '利率', '10', '期', '14', '投注', '12', '彩票', '本期', '主场', '开出', '万', '双色球', '30', '奖金', '注', '13', '号', '联赛', '开奖', '号码', '15', '客场', '微博', '31', '球队', '胜', '奖', '16', '平', '两队', '21', '中奖', '球', '一等奖', '场', '看好', '17', '大奖', '25', '负', '主队', '次', '本场', '走势', 'VS', '19', '18', '买', '头奖', '09', '23', '评论', '我要', '户型', '样板间', '论坛', '相册', '地图搜索', '平米', '开发商', '点评', '均价', '置业', '公布', '居', '为准', '房', '仅供参考', '编辑', '建筑', '热盘', '90', '楼盘', '入住', '以下', '社区', '面积', '动向', '订阅', '乐居', '发送到', '邮箱', '更多', '刊', '交通', '别墅', '升值', '开盘', '火爆', '潜力', '大兴', '折扣', '住宅', '环境', '在售', '一步到位', '居住', '70', '全能', '建筑面积', '总', '显示', '投资', '出现', '基金', '亿元', '股票', '数据', '证券', '股', '今年', '分别', '大盘', '上涨', '大幅', '下跌', '只', '个股', 'A股', '点', '明显', '占', '增长', '近期', '买入', '行情', '统计', '家', '去年', '沪', '涨幅', '指数', '股市', '持有', '股价', '反弹', '连续', '月份', '上市公司', '昨日', '低', '调整', '达', '估值', '比例', '股东', '交易日', '万股', '减持', '下降', '平均', '家居', '家装', '请', '房产', '点击', '火热', '装修论坛', '精彩内容', '2009', '装修', '家具', '装饰', '环保', '材料', '营销', '地板', '图为', '建材', '厨房', '卫浴', '色彩', '橱柜', '经销商', '厂家', '业', '店', '陶瓷', '安装', '家电', '沙发', '专卖店', '绿色', '木材', '节能', '施工', '木地板', '展会', '客厅', '颜色', '实木', '起到', '元素', '简约', '扩张', '谢谢', '实', '墙面', '销售额', '展', '原材料', '自考', '考试', '考生', '自学', '教育', '2010', '课程', '报考', '自考办', '报名', '考试院', '通知', '查询', '高等教育', '办理', '规定', '学习', '安排', '准考证', '毕业', '开考', '网站', '专科', '本科', '手续', 'www', '申请', '负责人', '市', 'cn', '提醒', '报讯', '地点', '说明', '本次', '新生', '教材', '主考', '非', '身份证', '日至', '领取', '实习', '登录', '招生', '考试成绩', '毕业生', '下半年', '考场', '考核', '3G', '表示', '移动', '消息', '已', '网络', '运营商', '联通', '中国联通', '元', '电信', '中国移动', 'iPhone', '透露', '通信', '中国电信', 'TD', '智能手机', '上市', '正式', '网', '厂商', '苹果', '此次', '互联网', '套餐', '无线', '近日', '此前', '2G', '建设', '话费', '预计', '补贴', '康钊', '上网', '版', 'LTE', '产业链', '月底', 'WCDMA', 'CDMA', '款', '智能', '营业厅', '合约', '千元', '万户', '用户数', '华为', '后', '记者', '时', '岁', '里', '发现', '看到', '却', '当时', '知道', '告诉', '一名', '20', '找', '钱', '走', '小', '孩子', '有人', '找到', '来到', '采访', '电话', '下午', '随后', '只能', '家里', '医院', '一天', '附近', '儿子', '每天', '住', '警方', '左右', '小时', '突然', '几个', '离开', '一年', '问', '跑', '父亲', '晚上', '父母', '回家', '不到', '上午', '未', '母亲', '导语', '身体', '吃', '女性', '减肥', '食物', '身材', '运动', '脂肪', '性感', '瘦身', '饮食', '女星', '瘦', '体内', '皮肤', '喝', '水果', '热量', '人体', '含有', '体重', '营养', '疾病', '肌肉', '美丽', '食品', '美女', '维生素', '动作', '蔬菜', '部位', '食用', '月经', '蛋白质', '吸收', '腿', '胸部', '锻炼', '按摩', '血液', '含', '水', '娱乐圈', '症状', '腹部', '下面', '功效', '补充', '种', '月', '日', '中', '年', '说', '认为', '下', '前', '称', '报道', '发生', '新', '曾', '这种', '情况', '政治', '总统', '这是', '影响', '之后', '安全', '一次', '地区', '两个', '应该', '之间', '宣布', '使', '决定', '这次', '之前', '过去', '来说', '人们', '原因', '造成', '民众', '提出', '这一', '举行', '存在', '行动', '事件', '官员', '发表', '许多', '准备', '面临', '战争', '导致', '比赛', '亚运会', '11', '奥运会', '决赛', '最后', '对手', '亚运', '冠军', '金牌', '体育讯', '结束', '中国队', '参加', '最终', '选手', '成绩', '球员', '分', '运动员', '男子', '一场', '夺冠', '赛后', '教练', '队员', '训练', '战胜', '状态', '米', '赛场', '女子', '分钟', '奥运', '这场', '拿到', '队', '比分', '世锦赛', '进攻', '队友', '组', '韩国队', '球迷', '击败', '秒', '体育', '半决赛', '小组赛', '防守', '图', '人', '好', '星座', '最', '做', '喜欢', '想', '容易', '爱', '觉得', '爱情', '事情', '再', '感觉', '对方', '感情', '双子座', '处女座', '狮子座', '天蝎座', '白羊座', '天秤座', '水瓶座', '双鱼座', '巨蟹座', '表现', '一起', '金牛座', '事', '总是', '射手座', '魔羯座', '一种', '比较', '机会', '太', '身边', '星', '一点', '想要', '心理', '恋爱', '男人', '注意', '心情', '往往', '情绪', '女人', '紫微星', '游戏', '玩家', '2011', '平台', '网游', '网络游戏', '运营', '社交', '一款', 'http', '玩', '在线', 'CEO', 'ChinaJoy', '视频', 'net', 'chinajoy', '组委会', '网页', '手机游戏', 'QQ', '页面', 'Facebook', '数码', '下载', 'CJ', '虚拟', '大会', '玩游戏', '大赛', '腾讯', '角色', '分享', '3D', '盛大', '版本', 'PC', '动漫', '电脑', '官方网站', '广告', '盛会', '巨头', '题材', '演讲', '010', '上线', 'Cosplay', '【', '】', '组图', '新浪', '讯', '音乐', '歌迷', '演唱会', '歌手', '唱', '歌曲', '专辑', '文并', '演唱', '新专辑', '拍摄', '舞台', '举办', '歌', 'TUNGSTAR', '观众', '唱片', '笑', 'MV', '表演', '粉丝', '出席', '嘉宾', '台北', '拍', '邀请', '听歌', '偶像', '内地', '开唱', '唱歌', '艺人', '一首', '现身', '人气', '导演', '亮相', '好友', '首歌', '演绎', '经典', '电影', '到场', '新歌', '合唱', '乐坛', '图库']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "\n",
    "def get_MI(word, category, d_words_count, d_doc_count):\n",
    "    N = sum([v for v in d_doc_count.values()])\n",
    "    try:\n",
    "        A = d_words_count[word][category]\n",
    "    except KeyError:\n",
    "        A = 0 \n",
    "    C = d_doc_count[category] - A\n",
    "    B = N - d_doc_count[category]- A\n",
    "    D = N - C - B -A\n",
    "    return A*N/((A+C)*(A+B))\n",
    "\n",
    "def feature_selection(d_words_count,d_doc_count):\n",
    "    '''\n",
    "    依据互信息最大的选择500个特征,返回特征语料库\n",
    "    '''\n",
    "#     d_words_count = {'财经':{},'彩票':{},'房产':{},'股票':{},'家居':{},'教育':{},'科技':{},'社会':{},'时尚':{},'时政':{}\n",
    "#                      ,'体育':{},'星座':{},'游戏':{},'娱乐':{}}  # {'财经': {'油价': 23, '高企将': 1,...\n",
    "    \n",
    "    l_corpus = []    \n",
    "    # 计算互信息\n",
    "    catedory_word_MIs = {'财经':[],'彩票':[],'房产':[],'股票':[],'家居':[],'教育':[],'科技':[],'社会':[],'时尚':[],'时政':[],\n",
    "                      '体育':[],'星座':[],'游戏':[],'娱乐':[]}\n",
    "    \n",
    "    \n",
    "    for word in d_words_count.keys():\n",
    "        max_MI, closest_category = 0, None\n",
    "        for category in d_doc_count.keys():\n",
    "            MI = get_MI(word,category,d_words_count, d_doc_count)\n",
    "            if MI > max_MI:\n",
    "                max_MI = MI \n",
    "                closest_category = category\n",
    "        catedory_word_MIs[closest_category].append((word,max_MI)) \n",
    "    \n",
    "    # 对各个类别选取前500个作为特征词\n",
    "    for category in d_doc_count.keys():\n",
    "        l_corpus.extend([x[0] for x in sorted(catedory_word_MIs[category], key=lambda x:x[1], reverse=True)[:50]])\n",
    "    \n",
    "    return l_corpus\n",
    "    \n",
    "def weight_calculation(l_corpus,l_words_counters,d_words_count,d_doc_count):\n",
    "    '''\n",
    "    依据TF-IDF值计算每个特征在分类中的权重，在贝叶斯中没用到\n",
    "    '''\n",
    "    all_counter = collections.Counter()\n",
    "    for counter in l_words_counters:\n",
    "        all_counter += counter\n",
    "    \n",
    "    N = sum([v for v in d_doc_count.values()])  # 总的文档数\n",
    "    word_tf_idf = {}\n",
    "    for word in l_corpus:\n",
    "        idf = math.log(N/sum([v for v in d_words_count[word].values()]))\n",
    "        word_tf_idf[word] = all_counter.get(word)*idf\n",
    "    \n",
    "    return word_tf_idf\n",
    "    \n",
    "    \n",
    "def feature_extraction():\n",
    "    '''\n",
    "    遍历train.txt，统计每个词在每个类别中出现次数，利用互信息选择特征，TF-IDF值计算特征权重\n",
    "    '''\n",
    "    info = info_statis()\n",
    "    l_corpus = feature_selection(info['d_words_count'], info['d_doc_count'])\n",
    "#     d_features = weight_calculation(l_corpus,info['l_words_counters'],info['d_words_count'],info['d_doc_count']) # 每个特征及其对应的权重\n",
    "#     return d_features\n",
    "    return l_corpus\n",
    "\n",
    "def save_features(features):\n",
    "    with open('features.json','w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(features))\n",
    "features = feature_extraction()\n",
    "save_features(features)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练分类器\n",
    "- 贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'财经': 850, '彩票': 850, '房产': 850, '股票': 850, '家居': 850, '教育': 850, '科技': 850, '社会': 850, '时尚': 850, '时政': 850, '体育': 850, '星座': 850, '游戏': 850, '娱乐': 850}\n"
     ]
    }
   ],
   "source": [
    "C = 14  # 设定的类的数量\n",
    "\n",
    "import collections\n",
    "\n",
    "def load_features():\n",
    "    with open('features.json',encoding='utf-8') as f:\n",
    "        features = json.loads(f.read())\n",
    "    return features\n",
    "\n",
    "def fit_transform(features, words_counter):\n",
    "    '''\n",
    "    将输入的文档依据特征集合转换为向量形式\n",
    "    '''\n",
    "    doc_feature_vector = []\n",
    "    for feature in features:\n",
    "        if words_counter.get(feature):\n",
    "            doc_feature_vector.append(words_counter.get(feature))\n",
    "        else:\n",
    "            doc_feature_vector.append(0)\n",
    "    return doc_feature_vector\n",
    "\n",
    "def transform(features, file_path):\n",
    "    '''\n",
    "    接受特征字典{'feature':weight}和文件（训练或测试），将数据转换为向量表示返回\n",
    "    '''\n",
    "    d_categories = {}  # 统计每个类的数量信息\n",
    "    l_category_vectors = []  # 统计每个类和特征向量信息\n",
    "    with open(file_path, encoding='UTF-8-sig') as f:\n",
    "        for line in f:\n",
    "            label, content = line.strip().split(' ',1)\n",
    "            words_counter = collections.Counter(content.split(' '))\n",
    "            doc_feature_vector = fit_transform(features, words_counter)\n",
    "            d_category_vector = {\n",
    "                'category':label,\n",
    "                'vector':doc_feature_vector\n",
    "            }\n",
    "            l_category_vectors.append(d_category_vector)\n",
    "            try:\n",
    "                d_categories[label] += 1\n",
    "            except KeyError:\n",
    "                d_categories[label] = 1\n",
    "            \n",
    "        return {\n",
    "            'd_categories':d_categories,  # 每个类别的统计信息，用于计算先验概率\n",
    "            'l_category_vectors':l_category_vectors  # 由类别和特征向量组成的字典列表\n",
    "        }\n",
    "           \n",
    "features = load_features()\n",
    "trans_res = transform(features,'train.txt')\n",
    "print(trans_res['d_categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T04:14:07.894211500Z",
     "start_time": "2024-04-01T04:14:07.192239600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training..\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 305\u001B[0m\n\u001B[0;32m    291\u001B[0m TEST_TEXT \u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;124m婚恋心理：婆媳聊天必知的潜规则(图)\u001B[39m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;124m　　婆媳矛盾是我们中华民族的千古矛盾，一直都得不到缓解。这个社会的人都有两面性，大家嘴上说着一套一套的漂亮话，但是实际上所作所为，又是另外一回事。而婆媳关系也有两套规则，一套是明规则，还有一套潜规则，利用好了，这个千古矛盾对你来说将不再是难题。\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;124m　　如果娘家和婆家势力悬殊，或是先生靠着爸爸提携，你就不用担心什么婆媳关系了，婆婆哪还敢说你坏话她得为儿子好啊。这种情况下，媳妇若是为长久计，就要锦上添花，待公婆好一些，省得老公翅膀硬了老爹退休了，公婆甚至老公一口恶气吐到脸上来。如果不想费力气，那也不用做什么，大家场面上过的去就行了。\u001B[39m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 305\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    306\u001B[0m     test()\n\u001B[0;32m    307\u001B[0m     predict(TEST_TEXT)\n",
      "Cell \u001B[1;32mIn[1], line 280\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m():\n\u001B[0;32m    279\u001B[0m     naive_bayes \u001B[38;5;241m=\u001B[39m NaiveBayes()\n\u001B[1;32m--> 280\u001B[0m     \u001B[43mnaive_bayes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    281\u001B[0m     save_model(naive_bayes,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbayes_model.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[1], line 245\u001B[0m, in \u001B[0;36mNaiveBayes.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    244\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining..\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 245\u001B[0m     transform_res \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_features,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_path)\n\u001B[0;32m    246\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prior_prb \u001B[38;5;241m=\u001B[39m get_prior_prb(transform_res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124md_categories\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m    247\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_likelihood_prb \u001B[38;5;241m=\u001B[39m get_likelihood_prb(transform_res[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml_category_vectors\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import jieba\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import prettytable as pt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "cag2index = {\n",
    "    '体育': 0, \n",
    "    '娱乐': 1, \n",
    "    '家居': 2, \n",
    "    '彩票': 3, \n",
    "    '房产': 4, \n",
    "    '教育': 5, \n",
    "    '时尚': 6, \n",
    "    '时政': 7, \n",
    "    '星座': 8, \n",
    "    '游戏': 9, \n",
    "    '社会': 10, \n",
    "    '科技': 11, \n",
    "    '股票': 12, \n",
    "    '财经': 13\n",
    "}\n",
    "def save_model(model=None, file_path='bayes_model.pkl'):\n",
    "    '''\n",
    "    存储bayes模型\n",
    "    '''\n",
    "    do = True\n",
    "    if os.path.exists(file_path):\n",
    "        print('-'*5+'bayes模型已经存在，此操作会覆盖原有模型（不可恢复），请确认是否继续(y/n):',end='')\n",
    "        op = input()\n",
    "        if op == 'y':\n",
    "            do = True\n",
    "        else:\n",
    "            do = False\n",
    "    if do:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "            print('-'*30+'bayes模型已更新'+'-'*32)\n",
    "\n",
    "def load_model(file_path='bayes_model.pkl'):\n",
    "    '''\n",
    "    导入训练好的bayes模型\n",
    "    :param file_path:\n",
    "    '''\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logging.error('模型文件不存在，请先训练模型并存储！')\n",
    "        exit(404)\n",
    "        \n",
    "\n",
    "def load_stop_words(stop_words_path='stopword.txt',stop_signals={'\\n','\\u3000','\\xa0',' ',}):\n",
    "    with open(stop_words_path,encoding='UTF-8-sig') as f:\n",
    "        s_stop_words = set()\n",
    "        for line in f:\n",
    "            s_stop_words.add(line.strip())\n",
    "        s_stop_words.update(stop_signals)\n",
    "        return s_stop_words\n",
    "    \n",
    "def get_prior_prb(d_categories):\n",
    "    '''\n",
    "    计算先验概率，接受由所有分类以及对应数量组成的字典\n",
    "    '''\n",
    "    d_prior_prb = {}  # P(cj)\n",
    "    N_all = sum(d_categories.values())\n",
    "    for category,N_c in d_categories.items():\n",
    "        d_prior_prb[category] = (1+N_c)/(C+N_all)\n",
    "    \n",
    "    return d_prior_prb\n",
    "\n",
    "def get_likelihood_prb(l_category_vectors):\n",
    "    '''\n",
    "    计算似然概率，接受由category和特征向量组成的列表\n",
    "    '''\n",
    "    d_likelihood_prb = {}   # P(wi|cj)\n",
    "    d_feature_category = {}  # 统计某一特征对应分类的分量\n",
    "    d_all_category = {}  # 统计所有特征对应分类的分量\n",
    "    for d_category_vector in l_category_vectors:\n",
    "        category = d_category_vector['category']\n",
    "        vector = d_category_vector['vector']\n",
    "        for i,v in enumerate(vector):\n",
    "            try:\n",
    "                d_feature_category[(i,category)] += v\n",
    "            except KeyError:\n",
    "                d_feature_category[(i,category)] = v\n",
    "\n",
    "            finally:\n",
    "                if category not in d_all_category:\n",
    "                    d_all_category[category] = v\n",
    "                else:\n",
    "                    d_all_category[category] += v\n",
    "\n",
    "    M = len(vector)\n",
    "\n",
    "    for k,v in d_feature_category.items():\n",
    "        d_likelihood_prb[k] = (1+v)/(M + d_all_category[k[1]]) \n",
    "    return d_likelihood_prb\n",
    "\n",
    "def cal_confusion_matrix(prior_prb,likelihood_prb, categories, l_category_vectors):\n",
    "    '''\n",
    "    依据测试集数据计算混淆矩阵\n",
    "    '''\n",
    "    confusion_matrix = np.zeros((len(categories),len(categories)))  # 初始化混淆矩阵\n",
    "    d_accuracy = {}\n",
    "    count = 0 \n",
    "    total = len(l_category_vectors)\n",
    "    print('测试数据：{}条'.format(total))\n",
    "    for d_category_vector in l_category_vectors:\n",
    "        # 对每一条特征做分类\n",
    "        true_category = d_category_vector['category']\n",
    "        vector = d_category_vector['vector']\n",
    "        if true_category not in d_accuracy:\n",
    "            # 第一次接受该分类\n",
    "            d_accuracy[true_category] = {'count':0,'total':1,'accuracy':0}\n",
    "        else:\n",
    "            d_accuracy[true_category]['total'] += 1\n",
    "            \n",
    "        max_predict_prb = 0\n",
    "        predict_category = '科技'\n",
    "        for category in categories:\n",
    "            # 计算该文本在各个分类下的概率\n",
    "            prod_likelihood_prb = 1\n",
    "            for i,v in enumerate(vector):\n",
    "                # 对向量中每一个不为0的分量求概率\n",
    "                if v != 0 :\n",
    "                    prod_likelihood_prb *= likelihood_prb[(i,category)]\n",
    "            predict_prb = prior_prb[category]*prod_likelihood_prb\n",
    "#             print('category:{},prb:{}'.format(category, predict_prb))\n",
    "            if predict_prb > max_predict_prb:\n",
    "                predict_category = category  # 更新概率最大类别\n",
    "                max_predict_prb = predict_prb\n",
    "#         print('true_category:{},predict_category:{}'.format(true_category,predict_category))\n",
    "        # 混淆矩阵信息更改\n",
    "        confusion_matrix[cag2index[true_category]][cag2index[predict_category]] += 1  # 将true预测成predict的数量\n",
    "    return confusion_matrix\n",
    "\n",
    "def cal_eval_index(confusion_matrix):\n",
    "    '''\n",
    "    基于混淆矩阵计算各个评测指标，包括精度，召回率，F1\n",
    "    '''\n",
    "    l_recalls = []\n",
    "    l_precisions = []\n",
    "    l_f1s = []\n",
    "    l_amounts = []\n",
    "    for i in range(len(confusion_matrix[:,0])):\n",
    "        # 遍历每一列，计算每个类的精度,召回率，f1\n",
    "        amount = sum(confusion_matrix[i,:])\n",
    "        precision = confusion_matrix[i,i]/ sum(confusion_matrix[:,i])\n",
    "        recall = confusion_matrix[i,i] / amount\n",
    "        l_precisions.append(precision)\n",
    "        l_recalls.append(recall)\n",
    "        l_f1s.append(2*(precision*recall)/(precision+recall))\n",
    "        l_amounts.append(amount)\n",
    "    # 计算总体平均\n",
    "    l_precisions.append(sum(l_precisions)/len(l_precisions))\n",
    "    l_recalls.append(sum(l_recalls)/len(l_recalls))\n",
    "    l_f1s.append(sum(l_f1s)/len(l_f1s))\n",
    "    l_amounts.append(sum(l_amounts))\n",
    "    return {\n",
    "        'categories':['体育','娱乐','家居','彩票','房产','教育','时尚','时政','星座','游戏','社会','科技','股票','财经','总体'],\n",
    "        'recalls':l_recalls,\n",
    "        'precisions':l_precisions,\n",
    "        'f1s':l_f1s,\n",
    "        'amounts':l_amounts\n",
    "    }\n",
    "    \n",
    "\n",
    "class NaiveBayes: \n",
    "    \n",
    "    def __init__(self):\n",
    "        self._features = self.__load_features()\n",
    "        self._prior_prb = {}  # 各个分类的先验概率\n",
    "        self._likelihood_prb = {}  # 各个特征在不同分类下的概率\n",
    "        self._l_categories = ['财经','彩票','房产','股票','家居','教育','科技','社会','时尚','时政','体育','星座','游戏','娱乐']   \n",
    "        \n",
    "        self._train_path = 'train.txt'\n",
    "        self._test_path = 'test.txt'\n",
    "        \n",
    "        self._s_stop_words = None\n",
    "    \n",
    "    def __load_features(self):\n",
    "        with open('features.json',encoding='utf-8') as f:\n",
    "            features = json.loads(f.read())\n",
    "        return features\n",
    "    \n",
    "    def _tokenization(self,text):\n",
    "        return jieba.cut(text)\n",
    "    \n",
    "    def _fit_transform(self,words_counter):\n",
    "        '''\n",
    "        将输入的文档依据特征集合转换为向量形式\n",
    "        '''\n",
    "        doc_feature_vector = []\n",
    "        for feature in self._features:\n",
    "            if words_counter.get(feature):\n",
    "                # 计算tf-if\n",
    "                info = info_statis()\n",
    "                l_corpus = feature_selection(info['d_words_count'], info['d_doc_count'])\n",
    "                tfidf = weight_calculation(l_corpus,info['l_words_counters'],info['d_words_count'],info['d_doc_count'])\n",
    "                print(tfidf)\n",
    "                break\n",
    "                doc_feature_vector.append(words_counter.get(feature))\n",
    "            else:\n",
    "                doc_feature_vector.append(0)\n",
    "        return doc_feature_vector\n",
    "    \n",
    "    def _text_preprocess(self,text):\n",
    "        '''\n",
    "        预处理数据，返回文本的特征向量表达形式\n",
    "        '''\n",
    "        if not self._s_stop_words:\n",
    "            self._s_stop_words = load_stop_words()\n",
    "        token_res = []          \n",
    "        for word in self._tokenization(text): # 分词\n",
    "            if word not in self._s_stop_words:  # 去停用词\n",
    "                token_res.append(word)\n",
    "        words_counter = Counter(token_res)\n",
    "        return self._fit_transform(words_counter)\n",
    "    \n",
    "    def _show_confusion_matrix(self,confusion_matrix):\n",
    "        f, ax = plt.subplots(figsize=(12, 10))\n",
    "        ax = sns.heatmap(confusion_matrix, annot=True, robust=True,fmt='.20g', cmap=\"Oranges\", linewidths=0.5)\n",
    "        f.savefig(\"bayes_confusion_matrix.png\", bbox_inches='tight', dpi=200)\n",
    "        plt.show()\n",
    "        \n",
    "    def _show_test_results(self,eval_index,confusion_matrix):\n",
    "        tb = pt.PrettyTable()\n",
    "        tb.add_column('Category',eval_index['categories'])\n",
    "        tb.add_column('Precesion',eval_index['precisions'])\n",
    "        tb.add_column('Recall',eval_index['recalls'])\n",
    "        tb.add_column('F1', eval_index['f1s'])\n",
    "        tb.add_column('Amount',eval_index['amounts'])\n",
    "        \n",
    "        print(tb)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        print('training..')\n",
    "        transform_res = transform(self._features,self._train_path)\n",
    "        self._prior_prb = get_prior_prb(transform_res['d_categories'])\n",
    "        self._likelihood_prb = get_likelihood_prb(transform_res['l_category_vectors'])\n",
    "        \n",
    "        \n",
    "    def test(self):\n",
    "        print('testing...')\n",
    "        transform_res = transform(self._features, self._test_path)\n",
    "        confusion_matrix = cal_confusion_matrix(self._prior_prb, \n",
    "                                                self._likelihood_prb, \n",
    "                                                self._l_categories, \n",
    "                                                transform_res['l_category_vectors'])\n",
    "        eval_index = cal_eval_index(confusion_matrix)\n",
    "        self._show_confusion_matrix(confusion_matrix)\n",
    "        self._show_test_results(eval_index,confusion_matrix)\n",
    "        \n",
    "    def predict(self,text):\n",
    "        vector = self._text_preprocess(text)\n",
    "        max_predict_prb = 0\n",
    "        for category in self._l_categories:\n",
    "            # 计算该文本在各个分类下的概率\n",
    "            prod_likelihood_prb = 1\n",
    "            for i,v in enumerate(vector):\n",
    "                # 对向量中每一个不为0的分量求概率\n",
    "                if v != 0 :\n",
    "                    prod_likelihood_prb *= self._likelihood_prb[(i,category)]\n",
    "            predict_prb = self._prior_prb[category]*prod_likelihood_prb\n",
    "#             print('category:{},prb:{}'.format(category, predict_prb))\n",
    "            if predict_prb > max_predict_prb:\n",
    "                predict_category = category  # 更新概率最大类别\n",
    "                max_predict_prb = predict_prb\n",
    "        return predict_category\n",
    "\n",
    "def train():\n",
    "    naive_bayes = NaiveBayes()\n",
    "    naive_bayes.train()\n",
    "    save_model(naive_bayes,'bayes_model.pkl')\n",
    "\n",
    "def test():\n",
    "    naive_bayes = load_model('bayes_model.pkl')\n",
    "    naive_bayes.test()\n",
    "\n",
    "def predict(text):\n",
    "    naive_bayes = load_model('bayes_model.pkl')\n",
    "    naive_bayes.predict(TEST_TEXT)\n",
    "    \n",
    "TEST_TEXT =\"\"\"\n",
    "婚恋心理：婆媳聊天必知的潜规则(图)\n",
    "　　婆媳矛盾是我们中华民族的千古矛盾，一直都得不到缓解。这个社会的人都有两面性，大家嘴上说着一套一套的漂亮话，但是实际上所作所为，又是另外一回事。而婆媳关系也有两套规则，一套是明规则，还有一套潜规则，利用好了，这个千古矛盾对你来说将不再是难题。\n",
    "　　婆媳相处如何妙用“潜规则”\n",
    "　　可是，我们当中有多少人是口含银匙而生呢？多少人是公主下嫁招驸马的童话呢我们当中的大多数，不都是要为柴米油盐生计而喜怒哀乐吗？不都要正视如何和婆家人相处——我们不想可又不得不去做吗？\n",
    "　　首先，我建议婆媳之间不要直接交流，有什么相左的意见应该通过先生缓冲一下，他是“汉奸”——会和皇军交流，也懂八路的心思。\n",
    "　　其次，如果直接交流受阻，一定要先自省：自己冲不冲动，有没有言语不当的地方，对婆婆有没有肢体冲撞，自己如果和妈妈这么说，妈妈怎么反应如果这些都自省过了，没有问题，那就要和先生说，实事求是，注意方式，不要动怒说粗，宁可哭，不可以骂人。如果是自己做的过分，有形式上的不当之处，但是内容没有错，就得避重就轻一点了，但是要提。记住：提前招认，绝对好过后来被盘问不得不招。如果你有重大错误，对不起，我也不知道怎么办了。因为我从来不和婆婆正面交流不同意见。请其她姐妹指点吧。\n",
    "　　总之，要做和先生解释说明冲突的第一人，要尽量心平气和，决不能搞人身攻击，婆婆丰满说人家是吹了气的青蛙，公公苗条说人家是榨干的甘蔗。要会做人，尤其是有外人在的场合，要表现的温和有礼，听话勤快，既让婆婆有面子，也可以请外人给你制造舆论。\n",
    "　　婆媳相处，要善于利用“潜规则”\n",
    "　　婆媳交流，要注意不能乱用潜规则，尽量说漂亮的官话。哪怕虚伪点，也不能来个赤裸裸的大实话，起码，不能首先使用大实话。聪明的妈妈会教女儿嘴巴要甜，说白了就是要会说官话。\n",
    "　　当然，官话不仅仅是说话，还包括行动。例如一个五十多岁的媳妇得到了众人的赞扬，说她有媳妇相，自己都是有媳妇的人了，还那么孝顺婆婆。那她是怎么做的呢有客人来了，她贴身伺候婆婆，给婆婆拿着热水袋，香烟火柴，站在婆婆身边伺候着。其实，她这是在监视婆婆，让她没法说坏话，要说，只能说好话——这样，她的好名声就得到最权威的认可了。\n",
    "　　如果娘家和婆家势力悬殊，或是先生靠着爸爸提携，你就不用担心什么婆媳关系了，婆婆哪还敢说你坏话她得为儿子好啊。这种情况下，媳妇若是为长久计，就要锦上添花，待公婆好一些，省得老公翅膀硬了老爹退休了，公婆甚至老公一口恶气吐到脸上来。如果不想费力气，那也不用做什么，大家场面上过的去就行了。\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "    test()\n",
    "    predict(TEST_TEXT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
